{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:41.458013Z",
          "iopub.status.busy": "2023-08-10T22:20:41.457237Z",
          "iopub.status.idle": "2023-08-10T22:20:42.748021Z",
          "shell.execute_reply": "2023-08-10T22:20:42.747111Z",
          "shell.execute_reply.started": "2023-08-10T22:20:41.457971Z"
        },
        "id": "YNr9Q7xixvQd",
        "trusted": true,
        "outputId": "a1ddb292-a6a9-4230-cfef-a6cac717735c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-12 12:53:42.820008: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-12 12:53:42.820038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-12 12:53:42.821168: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-12 12:53:42.827628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/home/santhosh/miniconda3/envs/tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import nltk\n",
        "\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "\n",
        "# from skimage.transform import resize\n",
        "# from skimage import io\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# tf.keras.utils.set_random_seed(1234)\n",
        "\n",
        "MAX_SENTENCE_LEN = 40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.750326Z",
          "iopub.status.busy": "2023-08-10T22:20:42.749721Z",
          "iopub.status.idle": "2023-08-10T22:20:42.770394Z",
          "shell.execute_reply": "2023-08-10T22:20:42.769617Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.750294Z"
        },
        "id": "ya2S4RtmxvQl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    # remove unnecessary characters in sentences and v\n",
        "\n",
        "    text = text.lower().strip()\n",
        "    #Seperate ?.!, with spaces\n",
        "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
        "    #Replace extra spaces with a single space\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"there's\", \"there is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", text)\n",
        "\n",
        "    #Remove trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.772149Z",
          "iopub.status.busy": "2023-08-10T22:20:42.771834Z",
          "iopub.status.idle": "2023-08-10T22:20:42.795838Z",
          "shell.execute_reply": "2023-08-10T22:20:42.794991Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.772121Z"
        },
        "id": "lTtFVSXhxvQo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess(movie_lines, movie_convs, split_ratio, start_tok, end_tok, subword=False):\n",
        "    #map line ids to line/dialog\n",
        "    conv_map = {}\n",
        "    for line in movie_lines:\n",
        "        if len(line) != 0:\n",
        "            line_split = line.split(\" +++$+++ \")\n",
        "            conv_map[line_split[0]] = line_split[4]\n",
        "\n",
        "    #create list containing lists of conversations\n",
        "    convid_list = []\n",
        "    for line in movie_convs:\n",
        "        if len(line) != 0:\n",
        "            conv = line.split(\" +++$+++ \")[-1][1:-1].strip(\"'\").split(\"', '\")\n",
        "            convid_list.append(conv)\n",
        "\n",
        "    #split into questions and answers\n",
        "    input, response  = [], []\n",
        "\n",
        "    for conv in convid_list:\n",
        "        for i in range(len(conv)-1):\n",
        "            input.append(clean_text(conv_map[conv[i]]))\n",
        "            response.append(clean_text(conv_map[conv[i+1]]))\n",
        "\n",
        "    #Segregating sentences which habe less than or eqqual to 100 words for faster training\n",
        "    filtered_input, filtered_response = [], []\n",
        "\n",
        "    num_qnans_pairs = len(input)\n",
        "\n",
        "    if not subword:\n",
        "        for i in range(num_qnans_pairs):\n",
        "            if len(input[i].split()) <= MAX_SENTENCE_LEN-2 and len(response[i].split()) <= MAX_SENTENCE_LEN-2:\n",
        "                    filtered_input.append(start_tok + \" \" + input[i] + \" \" + end_tok)\n",
        "                    filtered_response.append(start_tok + \" \" + response[i] + \" \" + end_tok)\n",
        "    else:\n",
        "        for i in range(num_qnans_pairs):\n",
        "            if len(input[i].split()) <= MAX_SENTENCE_LEN-2 and len(response[i].split()) <= MAX_SENTENCE_LEN-2:\n",
        "                    filtered_input.append(input[i])\n",
        "                    filtered_response.append(response[i])\n",
        "\n",
        "\n",
        "    #Split to training and test set\n",
        "    training_size = int(len(filtered_input) * split_ratio)\n",
        "\n",
        "    #Shuffe the qn answer pairs\n",
        "    idx = np.arange(len(filtered_input))\n",
        "    random.shuffle(idx)\n",
        "\n",
        "    shuffled_input, shuffled_response = [], []\n",
        "\n",
        "    for i in idx:\n",
        "        shuffled_input.append(filtered_input[i])\n",
        "        shuffled_response.append(filtered_response[i])\n",
        "\n",
        "    train_input, train_responses = shuffled_input[:training_size], shuffled_response[:training_size]\n",
        "    test_input, test_responses = shuffled_input[training_size:], shuffled_response[training_size:]\n",
        "\n",
        "    return (train_input, train_responses), (test_input, test_responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.798221Z",
          "iopub.status.busy": "2023-08-10T22:20:42.797927Z",
          "iopub.status.idle": "2023-08-10T22:20:42.812728Z",
          "shell.execute_reply": "2023-08-10T22:20:42.811880Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.798194Z"
        },
        "id": "n9Pm6JQkxvQq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def tokenize(train_inputs, train_outputs, test_inputs, test_outputs, oov_tok, num_words):\n",
        "    if num_words is not None:\n",
        "        tokenizer = Tokenizer(num_words=num_words, oov_token=oov_tok, lower=False, filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n',)\n",
        "    else:\n",
        "        tokenizer = Tokenizer(oov_token=oov_tok, lower=False)\n",
        "\n",
        "    tokenizer.fit_on_texts(train_inputs+train_outputs)\n",
        "\n",
        "    train_input_seq = tokenizer.texts_to_sequences(train_inputs)\n",
        "    train_output_seq = tokenizer.texts_to_sequences(train_outputs)\n",
        "\n",
        "    test_input_seq = tokenizer.texts_to_sequences(test_inputs)\n",
        "    test_output_seq = tokenizer.texts_to_sequences(test_outputs)\n",
        "\n",
        "    train_input_seq_pad = pad_sequences(train_input_seq, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "    train_output_seq_pad = pad_sequences(train_output_seq, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "\n",
        "    test_input_seq_pad = pad_sequences(test_input_seq, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "    test_output_seq_pad = pad_sequences(test_output_seq, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "\n",
        "    return (train_input_seq_pad, train_output_seq_pad), (test_input_seq_pad, test_output_seq_pad), tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.832986Z",
          "iopub.status.busy": "2023-08-10T22:20:42.832669Z",
          "iopub.status.idle": "2023-08-10T22:20:42.864261Z",
          "shell.execute_reply": "2023-08-10T22:20:42.863378Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.832958Z"
        },
        "id": "We_HGWmIxvQu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_dataset(train_in, train_out, batch_size):\n",
        "    #END token removed from decoder (as there's nothing to predict after the token) input and START token removed from output\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(({\"encoder_in\":train_in, \"decoder_in\":train_out[:,:-1]}, {\"outputs\": train_out[:, 1:]}))\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gM1Zz9awxQd"
      },
      "outputs": [],
      "source": [
        "def preprocess_testdata(sentence_list, tokenizer, start_token=None, end_token=None):\n",
        "    if start_token is not None:\n",
        "        for sentence in sentence_list:\n",
        "            sentence = start_token + \" \" + sentence + \" \" + end_token\n",
        "\n",
        "    test_tokens = tokenizer.texts_to_sequences(sentence_list)\n",
        "    pad_tokens = pad_sequences(test_tokens, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "    return pad_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EyDrvXrwxQe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#Data preprocessing constants\n",
        "SPLIT_RATIO = 0.9\n",
        "START_TOKEN, END_TOKEN = \"START\", \"END\"\n",
        "OOV_TOKEN = \"OOV\"\n",
        "\n",
        "\n",
        "#Transformer constants\n",
        "EMBEDDING_DIM = 128\n",
        "NUM_LAYERS = 4\n",
        "NUM_HEADS = 8\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "EPOCHS = 250\n",
        "NUM_WORDS = None\n",
        "TRAINING_SIZE = None\n",
        "\n",
        "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "\n",
        "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "BATCH_SIZE = 32 #* tpu_strategy.num_replicas_in_sync\n",
        "\n",
        "#open files and save data as variables\n",
        "with open('./archive/movie_lines.txt', encoding='utf-8', errors='ignore') as f:\n",
        "    movie_lines = f.read().split('\\n')\n",
        "\n",
        "with open('./archive/movie_conversations.txt', encoding='utf-8', errors='ignore') as f:\n",
        "    movie_convs = f.read().split('\\n')\n",
        "\n",
        "#preprocess and toknize data\n",
        "(train_sentences, train_sentences_outputs), (test_sentences, test_outputs) = preprocess(movie_lines, movie_convs, SPLIT_RATIO, START_TOKEN, END_TOKEN, subword=False)\n",
        "\n",
        "(train_inputs, train_outputs), (test_inputs, test_outputs), tokenizer = tokenize(train_sentences,\\\n",
        "                                                                                    train_sentences_outputs, test_sentences, test_outputs, OOV_TOKEN, NUM_WORDS)\n",
        "\n",
        "\n",
        "if NUM_WORDS is None:\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "else:\n",
        "    vocab_size = NUM_WORDS    #for word tokenizer\n",
        "    #vocab_size = NUM_WORDS + 2    #for subword tokenier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB4aR-tbwxQe"
      },
      "outputs": [],
      "source": [
        "f = open(\"chatbot_engine.trt\", \"rb\")\n",
        "runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n",
        "\n",
        "engine = runtime.deserialize_cuda_engine(f.read())\n",
        "context = engine.create_execution_context()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGMbv_quwxQf"
      },
      "outputs": [],
      "source": [
        "def predict(ip_batch, pred_tensor, output, d_input1, d_input2, d_output, bindings, stream):\n",
        "    cuda.memcpy_htod_async(d_input1, ip_batch, stream)\n",
        "    cuda.memcpy_htod_async(d_input2, pred_tensor, stream)\n",
        "\n",
        "    context.execute_async_v2(bindings, stream.handle, None)\n",
        "\n",
        "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
        "\n",
        "    stream.synchronize()\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFUB0trXwxQg"
      },
      "outputs": [],
      "source": [
        "def prediction_trt(input, query_sentences, tokenizer, start_token, end_token, output=None):\n",
        "\n",
        "    num_inputs = input.shape[0]\n",
        "\n",
        "    bleu_list = []\n",
        "    prediction_list = []\n",
        "    for _ in range(num_inputs):\n",
        "        prediction_list.append([start_token])\n",
        "\n",
        "    prediction_tensor = np.array(prediction_list, dtype=np.int32)\n",
        "\n",
        "    input = input.astype(np.int32)\n",
        "    #-------------------------------------------------------\n",
        "    d_input1 = cuda.mem_alloc(1*input.nbytes)\n",
        "    #-------------------------------------------------------\n",
        "\n",
        "\n",
        "    for i in range(MAX_SENTENCE_LEN):\n",
        "        #-------------------------------------------------------------\n",
        "        pred_shape = prediction_tensor.shape\n",
        "        output = np.empty([pred_shape[0], pred_shape[1], vocab_size], dtype=np.int32)\n",
        "        d_output = cuda.mem_alloc(1*output.nbytes)\n",
        "\n",
        "        d_input2 = cuda.mem_alloc(prediction_tensor.nbytes)\n",
        "        bindings = [int(d_input1), int(d_input2), int(d_output)]\n",
        "\n",
        "        stream = cuda.Stream()\n",
        "        #-------------------------------------------------------------\n",
        "        model_out = predict(input, prediction_tensor, output, d_input1, d_input2, d_output, bindings, stream)\n",
        "\n",
        "\n",
        "        last_words = model_out[:, -1:, :]\n",
        "\n",
        "        predicted_id = (np.argmax(last_words, axis=-1)).astype(np.int32)\n",
        "\n",
        "        # concatenated the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "\n",
        "        prediction_tensor = np.concatenate((prediction_tensor, predicted_id), axis=1)\n",
        "\n",
        "\n",
        "    for idx, pred in enumerate(prediction_tensor):\n",
        "        pred_tokens = []\n",
        "        for token in pred:\n",
        "            token_np = token #.numpy()\n",
        "            if(token_np == 0):\n",
        "                continue\n",
        "            if token_np != end_token:\n",
        "                word = tokenizer.index_word[token_np]\n",
        "                pred_tokens.append(word)\n",
        "            else:\n",
        "                break\n",
        "        pred_sentence = \" \".join(pred_tokens[1:])\n",
        "\n",
        "        query_words = query_sentences[idx].split(\" \")\n",
        "        query = \" \".join(query_words)\n",
        "\n",
        "        # print(f\"User: {query}\")\n",
        "        # print(f\"Chatbot: {pred_sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T23:39:50.735976Z",
          "iopub.status.busy": "2023-08-10T23:39:50.734873Z",
          "iopub.status.idle": "2023-08-10T23:40:13.624610Z",
          "shell.execute_reply": "2023-08-10T23:40:13.623291Z",
          "shell.execute_reply.started": "2023-08-10T23:39:50.735929Z"
        },
        "id": "vWlB9yvpxvRC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_input_sentences = [\"Hello, what's up?\", \"What is your plan?\", \"Are you going to the gym now?\", \"When is the book due\", \"What's the point?\", \"I'm visiting my parents tomorrow\", \"It'll be windy next week\", \"There is a storm tomorrow\"] *4\n",
        "test_inputs = preprocess_testdata(test_input_sentences, tokenizer, START_TOKEN, END_TOKEN)\n",
        "prediction_trt(test_inputs, test_input_sentences, tokenizer, tokenizer.word_index[START_TOKEN], tokenizer.word_index[END_TOKEN])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNsaeFTpwxQh",
        "outputId": "556a4f28-9227-4772-8f9b-46764e34ce61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.36 s ± 5.09 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "\n",
        "prediction_trt(test_inputs, test_input_sentences, tokenizer, tokenizer.word_index[START_TOKEN], tokenizer.word_index[END_TOKEN])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}