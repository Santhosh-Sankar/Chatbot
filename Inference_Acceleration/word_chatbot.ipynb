{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:41.458013Z",
          "iopub.status.busy": "2023-08-10T22:20:41.457237Z",
          "iopub.status.idle": "2023-08-10T22:20:42.748021Z",
          "shell.execute_reply": "2023-08-10T22:20:42.747111Z",
          "shell.execute_reply.started": "2023-08-10T22:20:41.457971Z"
        },
        "id": "YNr9Q7xixvQd",
        "trusted": true,
        "outputId": "8ce97a3b-e00b-4a78-f965-b8e30d5a08b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-12 15:55:07.526227: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-12 15:55:07.526461: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-12 15:55:07.787329: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-12 15:55:08.507519: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/home/santhosh/miniconda3/envs/tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import nltk\n",
        "\n",
        "tf.keras.utils.set_random_seed(1234)\n",
        "\n",
        "MAX_SENTENCE_LEN = 40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.750326Z",
          "iopub.status.busy": "2023-08-10T22:20:42.749721Z",
          "iopub.status.idle": "2023-08-10T22:20:42.770394Z",
          "shell.execute_reply": "2023-08-10T22:20:42.769617Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.750294Z"
        },
        "id": "ya2S4RtmxvQl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    # remove unnecessary characters in sentences and v\n",
        "\n",
        "    text = text.lower().strip()\n",
        "    #Seperate ?.!, with spaces\n",
        "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
        "    #Replace extra spaces with a single space\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"there's\", \"there is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", text)\n",
        "\n",
        "    #Remove trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.772149Z",
          "iopub.status.busy": "2023-08-10T22:20:42.771834Z",
          "iopub.status.idle": "2023-08-10T22:20:42.795838Z",
          "shell.execute_reply": "2023-08-10T22:20:42.794991Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.772121Z"
        },
        "id": "lTtFVSXhxvQo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess(movie_lines, movie_convs, split_ratio, start_tok, end_tok, subword=False):\n",
        "    #map line ids to line/dialog\n",
        "    conv_map = {}\n",
        "    for line in movie_lines:\n",
        "        if len(line) != 0:\n",
        "            line_split = line.split(\" +++$+++ \")\n",
        "            conv_map[line_split[0]] = line_split[4]\n",
        "\n",
        "    #create list containing lists of conversations\n",
        "    convid_list = []\n",
        "    for line in movie_convs:\n",
        "        if len(line) != 0:\n",
        "            conv = line.split(\" +++$+++ \")[-1][1:-1].strip(\"'\").split(\"', '\")\n",
        "            convid_list.append(conv)\n",
        "\n",
        "    #split into questions and answers\n",
        "    input, response  = [], []\n",
        "\n",
        "    for conv in convid_list:\n",
        "        for i in range(len(conv)-1):\n",
        "            input.append(clean_text(conv_map[conv[i]]))\n",
        "            response.append(clean_text(conv_map[conv[i+1]]))\n",
        "\n",
        "    #Segregating sentences which habe less than or eqqual to 100 words for faster training\n",
        "    filtered_input, filtered_response = [], []\n",
        "\n",
        "    num_qnans_pairs = len(input)\n",
        "\n",
        "    if not subword:\n",
        "        for i in range(num_qnans_pairs):\n",
        "            if len(input[i].split()) <= MAX_SENTENCE_LEN-2 and len(response[i].split()) <= MAX_SENTENCE_LEN-2:\n",
        "                    filtered_input.append(start_tok + \" \" + input[i] + \" \" + end_tok)\n",
        "                    filtered_response.append(start_tok + \" \" + response[i] + \" \" + end_tok)\n",
        "    else:\n",
        "        for i in range(num_qnans_pairs):\n",
        "            if len(input[i].split()) <= MAX_SENTENCE_LEN-2 and len(response[i].split()) <= MAX_SENTENCE_LEN-2:\n",
        "                    filtered_input.append(input[i])\n",
        "                    filtered_response.append(response[i])\n",
        "\n",
        "\n",
        "    #Split to training and test set\n",
        "    training_size = int(len(filtered_input) * split_ratio)\n",
        "\n",
        "    #Shuffe the qn answer pairs\n",
        "    idx = np.arange(len(filtered_input))\n",
        "    random.shuffle(idx)\n",
        "\n",
        "    shuffled_input, shuffled_response = [], []\n",
        "\n",
        "    for i in idx:\n",
        "        shuffled_input.append(filtered_input[i])\n",
        "        shuffled_response.append(filtered_response[i])\n",
        "\n",
        "    train_input, train_responses = shuffled_input[:training_size], shuffled_response[:training_size]\n",
        "    test_input, test_responses = shuffled_input[training_size:], shuffled_response[training_size:]\n",
        "\n",
        "    return (train_input, train_responses), (test_input, test_responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.798221Z",
          "iopub.status.busy": "2023-08-10T22:20:42.797927Z",
          "iopub.status.idle": "2023-08-10T22:20:42.812728Z",
          "shell.execute_reply": "2023-08-10T22:20:42.811880Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.798194Z"
        },
        "id": "n9Pm6JQkxvQq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def tokenize(train_inputs, train_outputs, test_inputs, test_outputs, oov_tok, num_words):\n",
        "    if num_words is not None:\n",
        "        tokenizer = Tokenizer(num_words=num_words, oov_token=oov_tok, lower=False, filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n',)\n",
        "    else:\n",
        "        tokenizer = Tokenizer(oov_token=oov_tok, lower=False)\n",
        "\n",
        "    tokenizer.fit_on_texts(train_inputs+train_outputs)\n",
        "\n",
        "    train_input_seq = tokenizer.texts_to_sequences(train_inputs)\n",
        "    train_output_seq = tokenizer.texts_to_sequences(train_outputs)\n",
        "\n",
        "    test_input_seq = tokenizer.texts_to_sequences(test_inputs)\n",
        "    test_output_seq = tokenizer.texts_to_sequences(test_outputs)\n",
        "\n",
        "    train_input_seq_pad = pad_sequences(train_input_seq, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "    train_output_seq_pad = pad_sequences(train_output_seq, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "\n",
        "    test_input_seq_pad = pad_sequences(test_input_seq, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "    test_output_seq_pad = pad_sequences(test_output_seq, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "\n",
        "    return (train_input_seq_pad, train_output_seq_pad), (test_input_seq_pad, test_output_seq_pad), tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.814151Z",
          "iopub.status.busy": "2023-08-10T22:20:42.813839Z",
          "iopub.status.idle": "2023-08-10T22:20:42.831528Z",
          "shell.execute_reply": "2023-08-10T22:20:42.830567Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.814123Z"
        },
        "id": "LBR5zfCUxvQs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def sub_tokenize(train_inputs, train_outputs, test_inputs, test_outputs, oov_tok, num_words):\n",
        "    # Build tokenizer using tfds for both questions and answers\n",
        "    tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "        train_inputs+train_outputs, target_vocab_size=num_words)\n",
        "\n",
        "    # Define start and end token to indicate the start and end of a sentence\n",
        "    START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "    train_in, train_out, test_in, test_out = [], [], [], []\n",
        "    for i in range(len(train_inputs)):\n",
        "        train_in.append(START_TOKEN + tokenizer.encode(train_inputs[i]) + END_TOKEN)\n",
        "        train_out.append(START_TOKEN + tokenizer.encode(train_outputs[i]) + END_TOKEN)\n",
        "\n",
        "    for i in range(len(test_inputs)):\n",
        "        test_in.append(START_TOKEN + tokenizer.encode(test_inputs[i]) + END_TOKEN)\n",
        "        test_out.append(START_TOKEN + tokenizer.encode(test_outputs[i]) + END_TOKEN)\n",
        "\n",
        "\n",
        "    pad_train_inputs = pad_sequences(train_in, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "    pad_train_outputs = pad_sequences(train_out, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "\n",
        "    pad_test_inputs = pad_sequences(test_in, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "    pad_test_outputs = pad_sequences(test_out, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "\n",
        "    return (pad_train_inputs, pad_train_outputs), (pad_test_inputs, pad_test_outputs), tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.832986Z",
          "iopub.status.busy": "2023-08-10T22:20:42.832669Z",
          "iopub.status.idle": "2023-08-10T22:20:42.864261Z",
          "shell.execute_reply": "2023-08-10T22:20:42.863378Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.832958Z"
        },
        "id": "We_HGWmIxvQu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_dataset(train_in, train_out, batch_size):\n",
        "    #END token removed from decoder (as there's nothing to predict after the token) input and START token removed from output\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(({\"encoder_in\":train_in, \"decoder_in\":train_out[:,:-1]}, {\"outputs\": train_out[:, 1:]}))\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.865709Z",
          "iopub.status.busy": "2023-08-10T22:20:42.865382Z",
          "iopub.status.idle": "2023-08-10T22:20:42.886367Z",
          "shell.execute_reply": "2023-08-10T22:20:42.885485Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.865666Z"
        },
        "id": "gHAvgRXAxvQv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_pad_mask(input):\n",
        "\n",
        "    pad_mask = tf.cast(tf.math.equal(input, 0), tf.float32)\n",
        "    pad_mask = tf.expand_dims(tf.expand_dims(pad_mask, axis=1), axis=1)\n",
        "    return pad_mask\n",
        "\n",
        "def create_look_ahead_mask(input):\n",
        "\n",
        "    seq_len = tf.shape(input)[1]\n",
        "    pad_mask = create_pad_mask(input)\n",
        "\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    pad_and_look_ahead_mask = tf.maximum(pad_mask, look_ahead_mask)\n",
        "    return pad_and_look_ahead_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.887867Z",
          "iopub.status.busy": "2023-08-10T22:20:42.887548Z",
          "iopub.status.idle": "2023-08-10T22:20:42.910976Z",
          "shell.execute_reply": "2023-08-10T22:20:42.910055Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.887839Z"
        },
        "id": "fpT1o6SpxvQv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class multiHeadAttn_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, embedding_dim, **kwargs):\n",
        "        #check if nembedding dim divisible by mum_heads\n",
        "        assert embedding_dim%num_heads == 0\n",
        "\n",
        "        super(multiHeadAttn_layer, self).__init__(**kwargs)\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.embedding_dim_per_head = self.embedding_dim // self.num_heads\n",
        "\n",
        "        self.query_transform = tf.keras.layers.Dense(self.embedding_dim)\n",
        "        self.key_transform = tf.keras.layers.Dense(self.embedding_dim)\n",
        "        self.value_transform = tf.keras.layers.Dense(self.embedding_dim)\n",
        "\n",
        "        self.permute = tf.keras.layers.Permute((2, 1, 3))\n",
        "        self.dense = tf.keras.layers.Dense(self.embedding_dim)\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(multiHeadAttn_layer, self).get_config()\n",
        "\n",
        "        #Update config with new layer attributes to make loading models easier\n",
        "        config.update({\"num_heads\": self.num_heads, \"embedding_dim\": self.embedding_dim})\n",
        "\n",
        "        return config\n",
        "\n",
        "    def call(self, query, key, value, mask):\n",
        "\n",
        "        batch_size, q_seq_len, k_seq_len= tf.shape(query)[0], tf.shape(query)[1], tf.shape(key)[1]\n",
        "\n",
        "        #Transform key, query, value\n",
        "        query_transformed = self.query_transform(query)\n",
        "        key_transformed = self.key_transform(key)\n",
        "        value_transformed = self.value_transform(value)\n",
        "\n",
        "        #Reshape  and permute dimensions to perform dot product per head\n",
        "        query_per_head = tf.reshape(query_transformed, (batch_size, q_seq_len, self.num_heads, self.embedding_dim_per_head))\n",
        "        key_per_head = tf.reshape(key_transformed, (batch_size, k_seq_len, self.num_heads, self.embedding_dim_per_head))\n",
        "        value_per_head = tf.reshape(value_transformed, (batch_size, k_seq_len, self.num_heads, self.embedding_dim_per_head))\n",
        "\n",
        "        query_per_head = self.permute(query_per_head)\n",
        "        key_per_head = self.permute(key_per_head)\n",
        "        value_per_head = self.permute(value_per_head)\n",
        "\n",
        "        query_per_head = tf.reshape(query_per_head, (batch_size*self.num_heads, q_seq_len, self.embedding_dim_per_head))\n",
        "        key_per_head = tf.reshape(key_per_head, (batch_size*self.num_heads, k_seq_len, self.embedding_dim_per_head))\n",
        "        value_per_head = tf.reshape(value_per_head, (batch_size*self.num_heads, k_seq_len, self.embedding_dim_per_head))\n",
        "\n",
        "        #Dot product between key and query to find similarities\n",
        "        dot_prod = tf.matmul(query_per_head, key_per_head, transpose_b=True)/ tf.math.sqrt(tf.cast(self.embedding_dim_per_head, dtype=tf.float32))\n",
        "\n",
        "        #To avoid considering the padded tokens and future tokens\n",
        "        dot_prod_reshaped = tf.reshape(dot_prod, (batch_size, self.num_heads, q_seq_len, k_seq_len))\n",
        "\n",
        "        dot_prod_reshaped += mask * -1e9\n",
        "\n",
        "        dot_prod = tf.reshape(dot_prod_reshaped, (batch_size*self.num_heads, q_seq_len, k_seq_len))\n",
        "\n",
        "        #Findding attention weights\n",
        "        attn_weights = tf.nn.softmax(dot_prod, axis=-1)\n",
        "\n",
        "        attn_out = tf.matmul(attn_weights, value_per_head)\n",
        "\n",
        "        #Reshaping the output back to the original shape\n",
        "        attn_out_reshaped = tf.reshape(attn_out, (batch_size, self.num_heads, q_seq_len, self.embedding_dim_per_head))\n",
        "\n",
        "        attn_out_permuted = self.permute(attn_out_reshaped)\n",
        "\n",
        "        attn_out = tf.reshape(attn_out_permuted, (batch_size, q_seq_len, self.embedding_dim))\n",
        "\n",
        "        #Final linear dense layer\n",
        "        output = self.dense(attn_out)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.912401Z",
          "iopub.status.busy": "2023-08-10T22:20:42.912102Z",
          "iopub.status.idle": "2023-08-10T22:20:42.928716Z",
          "shell.execute_reply": "2023-08-10T22:20:42.927922Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.912373Z"
        },
        "id": "_khl64_6xvQx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, max_len=10000, **kwargs):\n",
        "        super(PositionalEncoding_layer, self).__init__(**kwargs)\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding_layer, self).get_config()\n",
        "        config.update({\"embedding_dim\": self.embedding_dim, \"max_len\": self.max_len})\n",
        "\n",
        "        return config\n",
        "\n",
        "    def call(self, input):\n",
        "        batch_size = tf.shape(input)[0]\n",
        "        seq_len = tf.shape(input)[1]\n",
        "\n",
        "        #denominator\n",
        "        den = self.max_len**(tf.range(self.embedding_dim, delta=2, dtype=tf.float32)/self.embedding_dim)\n",
        "        den_stacked = tf.expand_dims(tf.expand_dims(den, axis=0), axis=1)\n",
        "        den_stacked = tf.repeat(tf.repeat(den_stacked, repeats=seq_len, axis=1), repeats=batch_size, axis=0)\n",
        "\n",
        "        #numerator\n",
        "        num_stacked = tf.expand_dims(tf.expand_dims(tf.range(seq_len, dtype=tf.float32), axis=0), axis=2)\n",
        "        num_stacked = tf.repeat(num_stacked, repeats=batch_size, axis=0)\n",
        "\n",
        "        inner_term = num_stacked / den_stacked\n",
        "\n",
        "        postn_encoding = tf.stack([tf.sin(inner_term), tf.cos(inner_term)], axis=-1)\n",
        "\n",
        "        postn_encoding = tf.reshape(postn_encoding, (batch_size, seq_len, self.embedding_dim))\n",
        "\n",
        "        output = input + postn_encoding\n",
        "\n",
        "        # return postn_encoding\n",
        "        return output, postn_encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.933501Z",
          "iopub.status.busy": "2023-08-10T22:20:42.933194Z",
          "iopub.status.idle": "2023-08-10T22:20:42.944838Z",
          "shell.execute_reply": "2023-08-10T22:20:42.943999Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.933473Z"
        },
        "id": "Y4mpFzQ9xvQy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def postn_encoding_check():\n",
        "\n",
        "    input = tf.ones((2, 10, 64))\n",
        "    _, out = PositionalEncoding_layer(64)(input)\n",
        "\n",
        "    plt.pcolormesh(out[0])\n",
        "    plt.xlabel(\"Depth\")\n",
        "    plt.xlim((0, 64))\n",
        "    plt.ylabel(\"Position\")\n",
        "    plt.colorbar()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.946193Z",
          "iopub.status.busy": "2023-08-10T22:20:42.945898Z",
          "iopub.status.idle": "2023-08-10T22:20:42.958776Z",
          "shell.execute_reply": "2023-08-10T22:20:42.957939Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.946166Z"
        },
        "id": "etxj2vUYxvQz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class feed_forward_network(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_units, **kwargs):\n",
        "        super(feed_forward_network, self).__init__(**kwargs)\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_units = num_units\n",
        "\n",
        "        self.dense1 = tf.keras.layers.Dense(self.num_units, activation=tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(self.embedding_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(feed_forward_network, self).get_config()\n",
        "        config.update({\"embedding_dim\": self.embedding_dim, \"num_units\": self.num_units})\n",
        "        return config\n",
        "\n",
        "\n",
        "    def call(self, input):\n",
        "        dense_out1 = self.dense1(input)\n",
        "        dense_out2 = self.dense2(dense_out1)\n",
        "        return dense_out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.960243Z",
          "iopub.status.busy": "2023-08-10T22:20:42.959928Z",
          "iopub.status.idle": "2023-08-10T22:20:42.976352Z",
          "shell.execute_reply": "2023-08-10T22:20:42.975434Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.960214Z"
        },
        "id": "i3kOBUq8xvQ0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class encoder_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads, num_dense_units, dropout_rate, **kwargs):\n",
        "        super(encoder_layer, self).__init__(**kwargs)\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_dense_units = num_dense_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.multiheadAttn = multiHeadAttn_layer(self.num_heads, self.embedding_dim)\n",
        "        self.feed_forward = feed_forward_network(self.embedding_dim, self.num_dense_units)\n",
        "        self.dropout = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "        self.add = tf.keras.layers.Add()\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(encoder_layer, self).get_config()\n",
        "        config.update({\"embedding_dim\": self.embedding_dim, \"num_heads\": self.num_heads, \"num_dense_units\": self.num_dense_units, \"dropout_rate\": self.dropout_rate})\n",
        "        return config\n",
        "\n",
        "    def call(self, input, mask):\n",
        "\n",
        "        attn_out = self.multiheadAttn(input, input, input, mask)\n",
        "        dropout_out1 = self.dropout(attn_out)\n",
        "        res_out1 = self.add([input, dropout_out1])\n",
        "        norm_out1 = self.layernorm(res_out1)\n",
        "\n",
        "        feed_forward_out = self.feed_forward(norm_out1)\n",
        "        dropout_out2 = self.dropout(feed_forward_out)\n",
        "        res_out2 = self.add([norm_out1, dropout_out2])\n",
        "        norm_out2 = self.layernorm(res_out2)\n",
        "\n",
        "        return norm_out2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.977754Z",
          "iopub.status.busy": "2023-08-10T22:20:42.977436Z",
          "iopub.status.idle": "2023-08-10T22:20:42.993568Z",
          "shell.execute_reply": "2023-08-10T22:20:42.992542Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.977726Z"
        },
        "id": "N_ZPhHyBxvQ1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_encoding_layers, embedding_dim, num_heads, num_dense_units, dropout_rate, **kwargs):\n",
        "        super(Encoder, self).__init__(**kwargs)\n",
        "        self.num_encoding_layers =  num_encoding_layers\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_dense_units = num_dense_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.encoder_layers_list = [encoder_layer(self.embedding_dim, self.num_heads, self.num_dense_units, self.dropout_rate) for _ in range(self.num_encoding_layers)]\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Encoder, self).get_config()\n",
        "        config.update({\"num_encoding_layers\": self.num_encoding_layers, \"embedding_dim\": self.embedding_dim,\\\n",
        "                       \"num_heads\": self.num_heads, \"num_dense_units\": self.num_dense_units, \"dropout_rate\": self.dropout_rate})\n",
        "\n",
        "        return config\n",
        "\n",
        "    def call(self, input, mask):\n",
        "\n",
        "        x = input\n",
        "        for layer in self.encoder_layers_list:\n",
        "            x = layer(x, mask)\n",
        "        output = self.layernorm(x)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:42.995479Z",
          "iopub.status.busy": "2023-08-10T22:20:42.995170Z",
          "iopub.status.idle": "2023-08-10T22:20:43.024101Z",
          "shell.execute_reply": "2023-08-10T22:20:43.022971Z",
          "shell.execute_reply.started": "2023-08-10T22:20:42.995451Z"
        },
        "id": "EV_2npzHxvQ2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class decoder_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads, num_dense_units, dropout_rate, **kwargs):\n",
        "        super(decoder_layer, self).__init__(**kwargs)\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_dense_units = num_dense_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.multiHeadAttn_self = multiHeadAttn_layer(self.num_heads, self.embedding_dim)\n",
        "        self.multiHeadAttn_cross = multiHeadAttn_layer(self.num_heads, self.embedding_dim)\n",
        "\n",
        "        self.feed_forward = feed_forward_network(self.embedding_dim, self.num_dense_units)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "        self.add = tf.keras.layers.Add()\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(decoder_layer, self).get_config()\n",
        "        config.update({\"embedding_dim\": self.embedding_dim, \"num_heads\": self.num_heads, \"num_dense_units\": self.num_dense_units, \"dropout_rate\": self.dropout_rate})\n",
        "\n",
        "        return config\n",
        "\n",
        "    def call(self, input, encoder_output, look_ahead_mask, pad_mask):\n",
        "\n",
        "        self_attn_out = self.multiHeadAttn_self(input, input, input, look_ahead_mask)\n",
        "        dropout_out1 = self.dropout(self_attn_out)\n",
        "        res_out1 = self.add([input, dropout_out1])\n",
        "        norm_out1 = self.layernorm(res_out1)\n",
        "\n",
        "        cross_attn_out = self.multiHeadAttn_cross(norm_out1, encoder_output, encoder_output, pad_mask)\n",
        "        dropout_out2 = self.dropout(cross_attn_out)\n",
        "        res_out2 = self.add([norm_out1, dropout_out2])\n",
        "        norm_out2 = self.layernorm(res_out2)\n",
        "\n",
        "        feed_forward_out = self.feed_forward(norm_out2)\n",
        "        dropout_out3 = self.dropout(feed_forward_out)\n",
        "        res_out3 = self.add([norm_out2, dropout_out3])\n",
        "        norm_out3 = self.layernorm(res_out3)\n",
        "\n",
        "        return norm_out3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:43.025729Z",
          "iopub.status.busy": "2023-08-10T22:20:43.025381Z",
          "iopub.status.idle": "2023-08-10T22:20:43.057735Z",
          "shell.execute_reply": "2023-08-10T22:20:43.056872Z",
          "shell.execute_reply.started": "2023-08-10T22:20:43.025670Z"
        },
        "id": "Y3CGb1pJxvQ2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_decoding_layers, embedding_dim, num_heads, num_dense_units, dropout_rate, **kwargs):\n",
        "        super(Decoder, self).__init__(**kwargs)\n",
        "        self.num_decoding_layers =  num_decoding_layers\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_dense_units = num_dense_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.decoder_layers_list = [decoder_layer(self.embedding_dim, self.num_heads, self.num_dense_units, self.dropout_rate) for _ in range(self.num_decoding_layers)]\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Decoder, self).get_config()\n",
        "        config.update({\"num_decoding_layers\": self.num_decoding_layers, \"embedding_dim\": self.embedding_dim,\\\n",
        "                       \"num_heads\": self.num_heads, \"num_dense_units\": self.num_dense_units, \"dropout_rate\": self.dropout_rate})\n",
        "\n",
        "        return config\n",
        "\n",
        "    def call(self, input, encoder_output, look_ahead_mask, pad_mask):\n",
        "\n",
        "        x = input\n",
        "        for layer in self.decoder_layers_list:\n",
        "            x = layer(x, encoder_output, look_ahead_mask, pad_mask)\n",
        "        output = self.layernorm(x)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:43.059238Z",
          "iopub.status.busy": "2023-08-10T22:20:43.058934Z",
          "iopub.status.idle": "2023-08-10T22:20:43.088514Z",
          "shell.execute_reply": "2023-08-10T22:20:43.087672Z",
          "shell.execute_reply.started": "2023-08-10T22:20:43.059211Z"
        },
        "id": "S-oO1V5cxvQ3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def Transformer(vocab_size, embedding_dim, num_layers, num_heads, num_dense_units, dropout_rate):\n",
        "\n",
        "    #Tokenized encoder and decoder inputs\n",
        "    encoder_inputs = tf.keras.Input(shape=(None,), name=\"encoder_in\")\n",
        "    decoder_inputs = tf.keras.Input(shape=(None,), name=\"decoder_in\")\n",
        "\n",
        "    #Create masks\n",
        "    encoder_pad_mask = tf.keras.layers.Lambda(create_pad_mask, output_shape=(1, 1, None))(encoder_inputs)\n",
        "    decoder_pad_mask = tf.keras.layers.Lambda(create_pad_mask, output_shape=(1, 1, None))(encoder_inputs)\n",
        "    decoder_look_ahead_mask=  tf.keras.layers.Lambda(create_look_ahead_mask, output_shape=(1, None, None))(decoder_inputs)\n",
        "\n",
        "    #Embed the inputs\n",
        "    embed_encoder_inputs = tf.keras.layers.Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "    embed_decoder_inputs = tf.keras.layers.Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "\n",
        "    #Positional Encoding\n",
        "    encoder_inputs_postn_encoded, _ = PositionalEncoding_layer(embedding_dim)(embed_encoder_inputs)\n",
        "    decoder_inputs_postn_encoded, _ = PositionalEncoding_layer(embedding_dim)(embed_decoder_inputs)\n",
        "\n",
        "    #Encoder\n",
        "    encoder_outputs = Encoder(num_layers, embedding_dim, num_heads, num_dense_units, dropout_rate)(encoder_inputs_postn_encoded, encoder_pad_mask)\n",
        "\n",
        "    #Decoder\n",
        "    decoder_outputs = Decoder(num_layers, embedding_dim, num_heads, num_dense_units, dropout_rate)(decoder_inputs_postn_encoded, encoder_outputs,\\\n",
        "                                                                                                   decoder_look_ahead_mask, decoder_pad_mask)\n",
        "\n",
        "    #Linear layer\n",
        "    logits = tf.keras.layers.Dense(vocab_size, name=\"outputs\")(decoder_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:43.089949Z",
          "iopub.status.busy": "2023-08-10T22:20:43.089599Z",
          "iopub.status.idle": "2023-08-10T22:20:43.110871Z",
          "shell.execute_reply": "2023-08-10T22:20:43.109996Z",
          "shell.execute_reply.started": "2023-08-10T22:20:43.089912Z"
        },
        "id": "5JXhplEIxvQ4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, MAX_SENTENCE_LEN - 1))\n",
        "\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:43.112322Z",
          "iopub.status.busy": "2023-08-10T22:20:43.112014Z",
          "iopub.status.idle": "2023-08-10T22:20:43.139972Z",
          "shell.execute_reply": "2023-08-10T22:20:43.139091Z",
          "shell.execute_reply.started": "2023-08-10T22:20:43.112294Z"
        },
        "id": "WG0DG0vkxvQ4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
        "    y_true = tf.reshape(y_true, shape=(-1, MAX_SENTENCE_LEN - 1))\n",
        "\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:43.141508Z",
          "iopub.status.busy": "2023-08-10T22:20:43.141114Z",
          "iopub.status.idle": "2023-08-10T22:20:43.152604Z",
          "shell.execute_reply": "2023-08-10T22:20:43.151739Z",
          "shell.execute_reply.started": "2023-08-10T22:20:43.141479Z"
        },
        "id": "3CTULOM_xvQ5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, embedding_dim, warmup_steps=3000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.embedding_dim = tf.constant(embedding_dim, dtype=tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"embedding_dim\": self.embedding_dim, \"warmup_steps\": self.warmup_steps}\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        lr = tf.math.multiply(tf.math.rsqrt(self.embedding_dim), tf.math.minimum(arg1, arg2))\n",
        "\n",
        "        return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T23:27:48.569880Z",
          "iopub.status.busy": "2023-08-10T23:27:48.568834Z",
          "iopub.status.idle": "2023-08-10T23:27:48.584168Z",
          "shell.execute_reply": "2023-08-10T23:27:48.582923Z",
          "shell.execute_reply.started": "2023-08-10T23:27:48.569824Z"
        },
        "id": "DNgHuTcnxvQ5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prediction(input, query_sentences, tokenizer, start_token, end_token, model, output=None):\n",
        "\n",
        "    num_inputs = input.shape[0]\n",
        "\n",
        "    bleu_list = []\n",
        "    prediction_list = []\n",
        "    for _ in range(num_inputs):\n",
        "        prediction_list.append([start_token])\n",
        "\n",
        "    prediction_tensor = tf.convert_to_tensor(prediction_list, dtype=tf.int32)\n",
        "\n",
        "    input = tf.convert_to_tensor(input, dtype=tf.int32)\n",
        "\n",
        "\n",
        "    for i in range(MAX_SENTENCE_LEN):\n",
        "        print(f\"Input shape: {input.shape}\\n\")\n",
        "        print(f\"pred tensor shape for {i}: {prediction_tensor.shape}\\n\")\n",
        "        model_out = model.predict([input, prediction_tensor], verbose=0)\n",
        "\n",
        "        print(f\"model_out shape for {i}: {model_out.shape} \\n\")\n",
        "\n",
        "        last_words = model_out[:, -1:, :]\n",
        "\n",
        "        #last_words = model_out[:, i:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(last_words, axis=-1), tf.int32)\n",
        "\n",
        "        # concatenated the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        prediction_tensor = tf.concat([prediction_tensor, predicted_id], axis=1)\n",
        "\n",
        "    for idx, pred in enumerate(prediction_tensor):\n",
        "        pred_tokens = []\n",
        "        for token in pred:\n",
        "            token_np = token.numpy()\n",
        "            if token_np != end_token:\n",
        "                word = tokenizer.index_word[token_np]\n",
        "                pred_tokens.append(word)\n",
        "            else:\n",
        "                break\n",
        "        pred_sentence = \" \".join(pred_tokens[1:])\n",
        "\n",
        "        query_words = query_sentences[idx].split(\" \")\n",
        "        query = \" \".join(query_words)\n",
        "\n",
        "        # print(f\"User: {query}\")\n",
        "        # print(f\"Chatbot: {pred_sentence}\")\n",
        "\n",
        "        if output is not None:\n",
        "            bleu_list.append(nltk.translate.bleu_score.sentence_bleu([output[idx].split(\" \")], pred_sentence.split(\" \")))\n",
        "\n",
        "\n",
        "    if output is not None:\n",
        "        print(sum(bleu_list)/len(bleu_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuhsB5RCCd8z"
      },
      "outputs": [],
      "source": [
        "def prediction_no_op(input, query_sentences, tokenizer, start_token, end_token, model, output=None):\n",
        "\n",
        "    num_inputs = input.shape[0]\n",
        "\n",
        "    bleu_list = []\n",
        "    prediction_list = []\n",
        "    for _ in range(num_inputs):\n",
        "        prediction_list.append([start_token])\n",
        "\n",
        "    prediction_tensor = tf.convert_to_tensor(prediction_list, dtype=tf.int32)\n",
        "\n",
        "    input = tf.convert_to_tensor(input, dtype=tf.int32)\n",
        "\n",
        "\n",
        "    for i in range(MAX_SENTENCE_LEN):\n",
        "\n",
        "        model_out = model.predict([input, prediction_tensor], verbose=0)\n",
        "\n",
        "        last_words = model_out[:, -1:, :]\n",
        "\n",
        "        #last_words = model_out[:, i:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(last_words, axis=-1), tf.int32)\n",
        "\n",
        "        # concatenated the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        prediction_tensor = tf.concat([prediction_tensor, predicted_id], axis=1)\n",
        "\n",
        "    for idx, pred in enumerate(prediction_tensor):\n",
        "        pred_tokens = []\n",
        "        for token in pred:\n",
        "            token_np = token.numpy()\n",
        "            if token_np != end_token:\n",
        "                word = tokenizer.index_word[token_np]\n",
        "                pred_tokens.append(word)\n",
        "            else:\n",
        "                break\n",
        "        pred_sentence = \" \".join(pred_tokens[1:])\n",
        "\n",
        "        query_words = query_sentences[idx].split(\" \")\n",
        "        query = \" \".join(query_words)\n",
        "\n",
        "        # print(f\"User: {query}\")\n",
        "        # print(f\"Chatbot: {pred_sentence}\")\n",
        "\n",
        "    #     if output is not None:\n",
        "    #         bleu_list.append(nltk.translate.bleu_score.sentence_bleu([output[idx].split(\" \")], pred_sentence.split(\" \")))\n",
        "\n",
        "\n",
        "    # if output is not None:\n",
        "    #     print(sum(bleu_list)/len(bleu_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:43.174811Z",
          "iopub.status.busy": "2023-08-10T22:20:43.174484Z",
          "iopub.status.idle": "2023-08-10T22:20:43.189414Z",
          "shell.execute_reply": "2023-08-10T22:20:43.188595Z",
          "shell.execute_reply.started": "2023-08-10T22:20:43.174782Z"
        },
        "id": "6WPtvVYTxvQ6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess_testdata(sentence_list, tokenizer, start_token=None, end_token=None):\n",
        "    if start_token is not None:\n",
        "        for sentence in sentence_list:\n",
        "            sentence = start_token + \" \" + sentence + \" \" + end_token\n",
        "\n",
        "    test_tokens = tokenizer.texts_to_sequences(sentence_list)\n",
        "    pad_tokens = pad_sequences(test_tokens, padding=\"post\", maxlen=MAX_SENTENCE_LEN)\n",
        "    return pad_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:43.191122Z",
          "iopub.status.busy": "2023-08-10T22:20:43.190537Z",
          "iopub.status.idle": "2023-08-10T22:20:43.203183Z",
          "shell.execute_reply": "2023-08-10T22:20:43.202314Z",
          "shell.execute_reply.started": "2023-08-10T22:20:43.191092Z"
        },
        "id": "1qRII3_NxvQ7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class TrainingStopCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # if epoch%100 == 0:\n",
        "        #     tf.keras.models.save_model(model, filepath=f\"./chatbot_{epoch}.h5\", include_optimizer=False)\n",
        "\n",
        "        if logs[\"accuracy\"] > 0.7:\n",
        "            print(\"\\n70% accuracy reached, training stopped!\\n\")\n",
        "            self.model.stop_training = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:20:43.204696Z",
          "iopub.status.busy": "2023-08-10T22:20:43.204304Z",
          "iopub.status.idle": "2023-08-10T22:21:36.664176Z",
          "shell.execute_reply": "2023-08-10T22:21:36.662940Z",
          "shell.execute_reply.started": "2023-08-10T22:20:43.204650Z"
        },
        "id": "UbCcVHeGxvQ7",
        "scrolled": true,
        "trusted": true,
        "outputId": "b29aacfb-2d09-408e-f51b-9a3546d4db93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-12 15:56:03.891288: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-12 15:56:04.752519: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-12 15:56:04.753213: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-12 15:56:04.770105: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-12 15:56:04.770892: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-12 15:56:04.771488: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-12 15:56:05.007774: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-12 15:56:05.008091: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-12 15:56:05.008382: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-12 15:56:05.008579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3485 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #Data preprocessing constants\n",
        "    SPLIT_RATIO = 0.9\n",
        "    START_TOKEN, END_TOKEN = \"START\", \"END\"\n",
        "    OOV_TOKEN = \"OOV\"\n",
        "\n",
        "\n",
        "    #Transformer constants\n",
        "    EMBEDDING_DIM = 128\n",
        "    NUM_LAYERS = 4\n",
        "    NUM_HEADS = 8\n",
        "    UNITS = 512\n",
        "    DROPOUT = 0.1\n",
        "    EPOCHS = 1\n",
        "    NUM_WORDS = None\n",
        "    TRAINING_SIZE = None\n",
        "\n",
        "    # tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "\n",
        "    # tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "    BATCH_SIZE = 32 #* tpu_strategy.num_replicas_in_sync\n",
        "\n",
        "    #open files and save data as variables\n",
        "    with open('./archive/movie_lines.txt', encoding='utf-8', errors='ignore') as f:\n",
        "        movie_lines = f.read().split('\\n')\n",
        "\n",
        "    with open('./archive/movie_conversations.txt', encoding='utf-8', errors='ignore') as f:\n",
        "        movie_convs = f.read().split('\\n')\n",
        "\n",
        "    #preprocess and toknize data\n",
        "    (train_sentences, train_sentences_outputs), (test_sentences, test_outputs) = preprocess(movie_lines, movie_convs, SPLIT_RATIO, START_TOKEN, END_TOKEN, subword=False)\n",
        "\n",
        "    (train_inputs, train_outputs), (test_inputs, test_outputs), tokenizer = tokenize(train_sentences,\\\n",
        "                                                                                     train_sentences_outputs, test_sentences, test_outputs, OOV_TOKEN, NUM_WORDS)\n",
        "\n",
        "    if TRAINING_SIZE is not None:\n",
        "        train_inputs, train_outputs = train_inputs[:TRAINING_SIZE, :], train_outputs[:TRAINING_SIZE, :]\n",
        "\n",
        "    #convert data to tf.data.Dataset\n",
        "    dataset = train_dataset(train_inputs, train_outputs, BATCH_SIZE)\n",
        "\n",
        "    # clear backend\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    if NUM_WORDS is None:\n",
        "        vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    else:\n",
        "        vocab_size = NUM_WORDS    #for word tokenizer\n",
        "        #vocab_size = NUM_WORDS + 2    #for subword tokenier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T22:21:36.671034Z",
          "iopub.status.busy": "2023-08-10T22:21:36.670742Z",
          "iopub.status.idle": "2023-08-10T22:21:43.599989Z",
          "shell.execute_reply": "2023-08-10T22:21:43.598818Z",
          "shell.execute_reply.started": "2023-08-10T22:21:36.671007Z"
        },
        "id": "sMaddOnvxvQ_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "learning_rate = CustomSchedule(EMBEDDING_DIM)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# initialize and compile model\n",
        "# with tpu_strategy.scope():\n",
        "model = Transformer(vocab_size, EMBEDDING_DIM, NUM_LAYERS, NUM_HEADS, UNITS, DROPOUT)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-08-10T22:21:43.601640Z",
          "iopub.status.busy": "2023-08-10T22:21:43.601307Z",
          "iopub.status.idle": "2023-08-10T23:21:40.583033Z",
          "shell.execute_reply": "2023-08-10T23:21:40.581663Z",
          "shell.execute_reply.started": "2023-08-10T22:21:43.601611Z"
        },
        "id": "pmJz7gDTxvRA",
        "outputId": "3fecc786-97fe-4198-f612-c565c2c4ad47",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-12 15:56:39.786638: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8900\n",
            "2024-02-12 15:56:40.571330: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f5924bb8de0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-02-12 15:56:40.571403: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1050, Compute Capability 6.1\n",
            "2024-02-12 15:56:40.752013: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1707771401.499694    9001 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5603/5603 [==============================] - 1077s 186ms/step - loss: 1.4465 - accuracy: 0.0416\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f59d2899f50>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_callback = TrainingStopCallback()\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1, callbacks=[stop_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80EQuMXkX1F7"
      },
      "outputs": [],
      "source": [
        "# model.save(\"chatbot\", include_optimizer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un0i7CPCfqRc"
      },
      "outputs": [],
      "source": [
        "# !zip -r /content/chatbot.zip /content/chatbot\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(\"/content/chatbot.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T23:27:59.341316Z",
          "iopub.status.busy": "2023-08-10T23:27:59.340879Z",
          "iopub.status.idle": "2023-08-10T23:36:42.448310Z",
          "shell.execute_reply": "2023-08-10T23:36:42.446916Z",
          "shell.execute_reply.started": "2023-08-10T23:27:59.341282Z"
        },
        "id": "7u4LH99pxvRB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "    # test_sentences_list = []\n",
        "    # gt_list = []\n",
        "    # for sent in train_sentences:\n",
        "    #     word_list = sent.split(\" \")\n",
        "    #     test_sentences_list.append(\" \".join(word_list[1:-1]))\n",
        "\n",
        "    # for sent in train_sentences_outputs:\n",
        "    #     word_list = sent.split(\" \")\n",
        "    #     gt_list.append(\" \".join(word_list[1:-1]))\n",
        "\n",
        "    # prediction(train_inputs[:50,:], test_sentences_list[:50], tokenizer, tokenizer.word_index[START_TOKEN], tokenizer.word_index[END_TOKEN], model, gt_list[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-10T23:39:50.735976Z",
          "iopub.status.busy": "2023-08-10T23:39:50.734873Z",
          "iopub.status.idle": "2023-08-10T23:40:13.624610Z",
          "shell.execute_reply": "2023-08-10T23:40:13.623291Z",
          "shell.execute_reply.started": "2023-08-10T23:39:50.735929Z"
        },
        "id": "vWlB9yvpxvRC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "    # test_input_sentences = [\"Hello, what's up?\", \"What is your plan?\", \"Are you going to the gym now?\", \"When is the book due\", \"What's the point?\", \"I'm visiting my parents tomorrow\", \"It'll be windy next week\", \"What do you want now\"] *4\n",
        "\n",
        "    # test_inputs = preprocess_testdata(test_input_sentences, tokenizer, START_TOKEN, END_TOKEN)\n",
        "    # prediction(test_inputs, test_input_sentences, tokenizer, tokenizer.word_index[START_TOKEN], tokenizer.word_index[END_TOKEN], model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GdEZyczE2DH",
        "outputId": "82cbe0a6-439f-460f-921c-7ba8094ff14e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello, what's up?\n",
            "Chatbot: i am not going to get a little\n",
            "User: What is your plan?\n",
            "Chatbot: i am not going to get a little\n",
            "User: Are you going to the gym now?\n",
            "Chatbot: i am not going to get a little\n",
            "User: When is the book due\n",
            "Chatbot: i am not going to get a little\n",
            "User: What's the point?\n",
            "Chatbot: i am not going to get a little\n",
            "User: I'm visiting my parents tomorrow\n",
            "Chatbot: i am not going to get a little\n",
            "User: It'll be windy next week\n",
            "Chatbot: i am not going to get a little\n",
            "User: What do you want now\n",
            "Chatbot: i am not going to get a little\n",
            "User: Hello, what's up?\n",
            "Chatbot: i am not going to get a little\n",
            "User: What is your plan?\n",
            "Chatbot: i am not going to get a little\n",
            "User: Are you going to the gym now?\n",
            "Chatbot: i am not going to get a little\n",
            "User: When is the book due\n",
            "Chatbot: i am not going to get a little\n",
            "User: What's the point?\n",
            "Chatbot: i am not going to get a little\n",
            "User: I'm visiting my parents tomorrow\n",
            "Chatbot: i am not going to get a little\n",
            "User: It'll be windy next week\n",
            "Chatbot: i am not going to get a little\n",
            "User: What do you want now\n",
            "Chatbot: i am not going to get a little\n",
            "User: Hello, what's up?\n",
            "Chatbot: i am not going to get a little\n",
            "User: What is your plan?\n",
            "Chatbot: i am not going to get a little\n",
            "User: Are you going to the gym now?\n",
            "Chatbot: i am not going to get a little\n",
            "User: When is the book due\n",
            "Chatbot: i am not going to get a little\n",
            "User: What's the point?\n",
            "Chatbot: i am not going to get a little\n",
            "User: I'm visiting my parents tomorrow\n",
            "Chatbot: i am not going to get a little\n",
            "User: It'll be windy next week\n",
            "Chatbot: i am not going to get a little\n",
            "User: What do you want now\n",
            "Chatbot: i am not going to get a little\n",
            "User: Hello, what's up?\n",
            "Chatbot: i am not going to get a little\n",
            "User: What is your plan?\n",
            "Chatbot: i am not going to get a little\n",
            "User: Are you going to the gym now?\n",
            "Chatbot: i am not going to get a little\n",
            "User: When is the book due\n",
            "Chatbot: i am not going to get a little\n",
            "User: What's the point?\n",
            "Chatbot: i am not going to get a little\n",
            "User: I'm visiting my parents tomorrow\n",
            "Chatbot: i am not going to get a little\n",
            "User: It'll be windy next week\n",
            "Chatbot: i am not going to get a little\n",
            "User: What do you want now\n",
            "Chatbot: i am not going to get a little\n"
          ]
        }
      ],
      "source": [
        "    test_input_sentences = [\"Hello, what's up?\", \"What is your plan?\", \"Are you going to the gym now?\", \"When is the book due\", \"What's the point?\", \"I'm visiting my parents tomorrow\", \"It'll be windy next week\", \"What do you want now\"] *4\n",
        "\n",
        "    test_inputs = preprocess_testdata(test_input_sentences, tokenizer, START_TOKEN, END_TOKEN)\n",
        "    prediction_trt(test_inputs, test_input_sentences, tokenizer, tokenizer.word_index[START_TOKEN], tokenizer.word_index[END_TOKEN], model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyPXqrSan_g0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "    test_input_sentences = [\"Hello, what's up?\", \"What is your plan?\", \"Are you going to the gym now?\", \"When is the book due\", \"What's the point?\", \"I'm visiting my parents tomorrow\", \"It'll be windy next week\", \"What do you want now\"] *4\n",
        "\n",
        "    test_inputs = preprocess_testdata(test_input_sentences, tokenizer, START_TOKEN, END_TOKEN)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlsGxdNew2mw",
        "outputId": "85e08226-b6c2-459d-a1d8-16cb39c3071e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8.26 s ± 126 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "\n",
        "    prediction_no_op(test_inputs, test_input_sentences, tokenizer, tokenizer.word_index[START_TOKEN], tokenizer.word_index[END_TOKEN], model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}